{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Baseline 1.0\n",
    "\n",
    "The plan is to build a WavLM_Base + ECAPA-TDNN model mainly to understand how the whole thing works. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11f824f89de81bce"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-15T14:06:21.796092056Z",
     "start_time": "2024-03-15T14:06:20.227623664Z"
    }
   },
   "outputs": [],
   "source": [
    "import torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## WavLM_Base\n",
    "\n",
    "- [docs](https://pytorch.org/audio/main/generated/torchaudio.pipelines.WAVLM_BASE.html)\n",
    "- [source](https://github.com/microsoft/unilm/tree/65f15af2a307ebb64cfb25adf54375b002e6fe8d/wavlm#pre-trained-models)\n",
    "- MIT license\n",
    "- pretrained on 960h of Librispeech @ 16kHz, make sure that your speech input is also sampled at 16kHz"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7397f2e7894dff8b"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "bundle = torchaudio.pipelines.WAVLM_BASE"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T14:06:21.800011538Z",
     "start_time": "2024-03-15T14:06:21.797241269Z"
    }
   },
   "id": "d5c9b8924b0d78eb"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/torchaudio/models/wavlm_base.pth\" to /home/milan/.cache/torch/hub/checkpoints/wavlm_base.pth\n",
      "6.5%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "16.7%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "33.0%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "54.3%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "75.6%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "84.4%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "92.8%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "# Construct the model and load the pretrained weight\n",
    "model = bundle.get_model()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T14:11:11.686185566Z",
     "start_time": "2024-03-15T14:10:23.808164177Z"
    }
   },
   "id": "3e81cacea5ebfffd"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "Wav2Vec2Model(\n  (feature_extractor): FeatureExtractor(\n    (conv_layers): ModuleList(\n      (0): ConvLayerBlock(\n        (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n      )\n      (1-4): 4 x ConvLayerBlock(\n        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n      )\n      (5-6): 2 x ConvLayerBlock(\n        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n      )\n    )\n  )\n  (encoder): Encoder(\n    (feature_projection): FeatureProjection(\n      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (projection): Linear(in_features=512, out_features=768, bias=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (pos_conv_embed): ConvolutionalPositionalEmbedding(\n        (conv): ParametrizedConv1d(\n          768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n          (parametrizations): ModuleDict(\n            (weight): ParametrizationList(\n              (0): _WeightNorm()\n            )\n          )\n        )\n      )\n      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n      (layers): ModuleList(\n        (0): EncoderLayer(\n          (attention): WavLMSelfAttention(\n            (rel_attn_embed): Embedding(320, 12)\n            (attention): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n            )\n            (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): FeedForward(\n            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (1-11): 11 x EncoderLayer(\n          (attention): WavLMSelfAttention(\n            (attention): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n            )\n            (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): FeedForward(\n            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n  )\n)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T14:15:56.102799226Z",
     "start_time": "2024-03-15T14:15:56.060721274Z"
    }
   },
   "id": "544cc06b3655cc0a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ECAPA-TDNN\n",
    "\n",
    "you need to feed into it the weighted average of all layers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c35ac93e18ad660b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# https://github.com/TaoRuijie/ECAPA-TDNN/blob/4904fda6c5da208998321e3ea75e24320bab8aad/model.py#L132C1-L132C6\n",
    "from ecapa_tdnn import ECAPA_TDNN\n",
    "# TODO look at layers after ECAPA_TDNN encoder"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efe66092c2e7b90b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the dataset\n",
    "\n",
    "Make sure it's 16MHz\n",
    "waveform = torchaudio.functional.resample(waveform, sample_rate, bundle.sample_rate)\n",
    "\n",
    "I can start training on Voxceleb1 & 2\n",
    "\n",
    "Besides, we also did data augmentation\n",
    "for the training data using the MUSAN [35] noise and RIR 1 rever-\n",
    "beration with probability 0.6 in online mode."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6af69c41f5946c9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Same as the implementation in [1], we also use the additive an-\n",
    "gular margin (AAM) [33] loss in the training process for model op-\n",
    "timization.\n",
    "AAM-softmax loss,\n",
    "margin 0.2\n",
    "\n",
    "https://paperswithcode.com/method/arcface\n",
    "\n",
    "ization.\n",
    "The training pipeline is mainly divided into two stages. In\n",
    "the first stage, the pre-trained model is fixed. We only update the\n",
    "ECAPA-TDNN and the weight w for all the hidden states. Then,\n",
    "we fine-tune all the parameters for pre-trained model and ECAPA-\n",
    "TDNN\n",
    "\n",
    "During the training process,\n",
    "we randomly sampled 3s segment from each utterance to construct\n",
    "training batch. For the two-stage training pipeline described in sec-\n",
    "tion 3.2.2, we first fixed the pre-trained model and trained for 10\n",
    "epochs. Then, we fine-tuned all the parameters for another 5 epochs.\n",
    "Besides, to further improve our best system, we did large margin\n",
    "fine-tuning [36] by randomly sampling 6s segments and set the AAM\n",
    "margin to 0.5 to train extra 2 epochs.\n",
    "\n",
    "During the evaluation, we use the cosine score to measure the\n",
    "similarity for trial pairs. We also use the adaptive s-norm [37, 38] to\n",
    "normalize the scores in our experiment. The embeddings extracted\n",
    "from the training set are averaged according to the speaker label and\n",
    "used as the imposter cohort. We set the imposter cohort size to 600\n",
    "in our experiment. When doing quality-aware score calibration [36],\n",
    "we randomly generated 30k trials based on the voxceleb2 test set to\n",
    "train our calibration model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47605446a4f5c6da"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "74f7a39530aa1312"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "387df210a1f8f4cf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f6d5948557c1cf09"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is good: https://github.com/microsoft/unilm/tree/65f15af2a307ebb64cfb25adf54375b002e6fe8d/wavlm#pre-trained-models\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d57038f16424372"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
